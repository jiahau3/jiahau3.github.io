<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Creating a Chatbot | JH&#39;s Blog</title>
<meta name="keywords" content="Sentiment Analysis, Word Embedding, LLM">
<meta name="description" content="What is a chatbot? A chatbot is an assistant for answering questions, saving time from waiting human&rsquo;s responses. But how? How computer can understand human language? Indeed, it can&rsquo;t read words like we do. It reads number.
Therefore, how to transform words into numbers is crucial for using languages to communicate with computers.
Data Preprocessing
Depending on what kind of chatbot to build, the collection of data can range from open source platform, scraping from websites, accessing to APIs from data vendor. It may includes labels for doing classification problems. If not, having some domain knowledge is important to do proper labelling.">
<meta name="author" content="Jia-Hau Ching">
<link rel="canonical" href="http://localhost:1313/posts/chatbot/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.d72444526d7ecbdb0015438a7fa89054a658bf759d0542e2e5df81ce94b493ee.css" integrity="sha256-1yREUm1&#43;y9sAFUOKf6iQVKZYv3WdBULi5d&#43;BzpS0k&#43;4=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/images/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/chatbot/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="JH&#39;s Blog (Alt + H)">JH&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/apps/" title="Apps">
                    <span>Apps</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Creating a Chatbot
    </h1>
    <div class="post-meta"><span title='2023-11-26 13:46:50 +0800 CST'>November 26, 2023</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Jia-Hau Ching

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#data-preprocessing" aria-label="Data Preprocessing">Data Preprocessing</a></li>
                <li>
                    <a href="#feature-extraction" aria-label="Feature Extraction">Feature Extraction</a><ul>
                        
                <li>
                    <a href="#bag-of-words" aria-label="Bag-of-Words">Bag-of-Words</a></li>
                <li>
                    <a href="#tf-idf" aria-label="TF-IDF">TF-IDF</a></li>
                <li>
                    <a href="#word-embedding" aria-label="Word Embedding">Word Embedding</a></li>
                <li>
                    <a href="#bert" aria-label="BERT">BERT</a></li>
                <li>
                    <a href="#llm" aria-label="LLM">LLM</a></li></ul>
                </li>
                <li>
                    <a href="#model-evaluation" aria-label="Model Evaluation">Model Evaluation</a></li>
                <li>
                    <a href="#deployment" aria-label="Deployment">Deployment</a></li>
                <li>
                    <a href="#citation" aria-label="Citation">Citation</a></li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>What is a chatbot? A chatbot is an assistant for answering questions, saving time from waiting human&rsquo;s responses. But how? How computer can understand human language? Indeed, it can&rsquo;t read words like we do. It reads number.
Therefore, how to transform words into numbers is crucial for using languages to communicate with computers.</p>
<h2 id="data-preprocessing">Data Preprocessing<a hidden class="anchor" aria-hidden="true" href="#data-preprocessing">#</a></h2>
<p>Depending on what kind of chatbot to build, the collection of data can range from open source platform, scraping from websites, accessing to APIs from data vendor. It may includes labels for doing classification problems. If not, having some domain knowledge is important to do proper labelling.</p>
<p>Then, based on what models to use, the data cleaning process also needs some trial and error. Nevertheless, it usually includes removing punctuation, stop words, and special characters. We can set up a rough preprocessing first and evaluate model performance by splitting data into training, validation and test set. This process iterates over and over until getting an acceptable model performance.</p>
<h2 id="feature-extraction">Feature Extraction<a hidden class="anchor" aria-hidden="true" href="#feature-extraction">#</a></h2>
<h3 id="bag-of-words">Bag-of-Words<a hidden class="anchor" aria-hidden="true" href="#bag-of-words">#</a></h3>
<p>There are some methods to transform words into numbers. For example, assigning each word with a unique number. Let&rsquo;s say if we have a sentence &ldquo;It is a beautiful day!&rdquo;. The sentence can be represented by numbers like &ldquo;0 1 2 3 4 5&rdquo;. This transformation is called encoding. The exact procedure is as follows</p>
<ol>
<li>Tokenize sentences</li>
<li>Create a vocabulary containing all words</li>
<li>Encode a sentence as a vector with size equivalent to vocabulary size</li>
</ol>
<p>Here is a snippet for demonstration</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> nltk.tokenize <span style="color:#f92672">import</span> word_tokenize
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">generate_BOW</span>(sentences: list, tokenizer):
</span></span><span style="display:flex;"><span>    vocab <span style="color:#f92672">=</span> set([j <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> sentences <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> tokenizer(i)])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> sentence <span style="color:#f92672">in</span> sentences:
</span></span><span style="display:flex;"><span>        words <span style="color:#f92672">=</span> tokenizer(sentence)
</span></span><span style="display:flex;"><span>        bag_vector <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(len(vocab))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> w <span style="color:#f92672">in</span> words:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> i, word <span style="color:#f92672">in</span> enumerate(vocab):
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">if</span> word <span style="color:#f92672">==</span> w:
</span></span><span style="display:flex;"><span>                    bag_vector[i] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>sentence<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{</span>bag_vector<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sentence_list <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;It is a beautiful day!&#39;</span>, <span style="color:#e6db74">&#39;Anyone wants to go out for an adventure?&#39;</span>, <span style="color:#e6db74">&#39;It is going to be interesting!&#39;</span>]
</span></span><span style="display:flex;"><span>generate_BOW(sentence_list, word_tokenize)
</span></span></code></pre></div><p>The encoded sentences are</p>
<pre tabindex="0"><code>It is a beautiful day!
[1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]

Anyone wants to go out for an adventure?
[0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0.]

It is going to be interesting!
[1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1.]
</code></pre><p>with vocabulary of this example</p>
<pre tabindex="0"><code>{&#39;It&#39;, &#39;an&#39;, &#39;day&#39;, &#39;adventure&#39;, &#39;?&#39;, &#39;going&#39;, &#39;a&#39;, &#39;is&#39;, &#39;to&#39;, &#39;for&#39;, &#39;be&#39;, &#39;Anyone&#39;, &#39;out&#39;, &#39;wants&#39;, &#39;interesting&#39;, &#39;beautiful&#39;, &#39;go&#39;, &#39;!&#39;}
</code></pre><p>This method is called Bag-of-Word algorithm. Since word/sentence is vectorized, we can use dot-product, cosine similarity as a metric to get the desired contents. It is usually used to compare similarity of sentences, documents. More frequent the word is, the bigger part it plays in the whole sentence/article. Here is a simple visualization by using wordcloud.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> collections <span style="color:#f92672">import</span> Counter
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> wordcloud <span style="color:#f92672">import</span> WordCloud
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> matplotlib <span style="color:#f92672">import</span> pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>tokenized_list <span style="color:#f92672">=</span> [ j <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> sentence_list <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> word_tokenize(i)]
</span></span><span style="display:flex;"><span>word_freq <span style="color:#f92672">=</span> Counter(tokenized_list)
</span></span><span style="display:flex;"><span>wordcloud <span style="color:#f92672">=</span> WordCloud(width<span style="color:#f92672">=</span><span style="color:#ae81ff">800</span>, height<span style="color:#f92672">=</span><span style="color:#ae81ff">400</span>, background_color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;white&#39;</span>)<span style="color:#f92672">.</span>generate_from_frequencies(word_freq)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Display the word cloud using matplotlib</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>,<span style="color:#ae81ff">5</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>imshow(wordcloud, interpolation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bilinear&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#34;off&#34;</span>)  <span style="color:#75715e"># Remove the axis</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()    
</span></span></code></pre></div><p><img alt="WordCloud" loading="lazy" src="/images/wordcloud.png"></p>
<h3 id="tf-idf">TF-IDF<a hidden class="anchor" aria-hidden="true" href="#tf-idf">#</a></h3>
<p>However, some cleaning is necessary for getting rid of trivial word or symbol. This process needs some domain knowledge to carefully get the ideal results. Another algorithm, Term Frequency - Inverse Document Frequency(TF-IDF) is suitable fro resolving this issue. As the phrase implies, it consists of two parts, TF and IDF [<a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">1</a>]. Term frequency, tf(t,d), is the relative frequency of term t within document d,
$$
tf(t, d) = \frac{f_{t,d}}{\Sigma_{t^{&rsquo;}\in d}f_{t^{&rsquo;},d}}
$$
where $f_{t,d}$ is the raw count of a term in a document, i.e., the number of times that term t occurs in document d.
$$
idf(t, D) = log(\frac{N}{|{d\in D:t\in d}|})
$$
with</p>
<ul>
<li>N: total number of documents in the corpus N = |D|</li>
<li>$|{d\in D:t\in d}|$: number of documents where the term t appears.</li>
</ul>
<p>The inverse document frequency is a measure of how much information the word provides, i.e., if it is common or rare across all documents.
Multiplication of these two terms gives weights to words, which distinguishes the importance between words in documents. This method is helpful for getting keywords in documents.</p>
<p>However, these two methods are not ideal for sentiment analysis problems since they may provide the same outcome with the sentences composed of same words with difference order.
Like &lsquo;Cancer destroying immune system&rsquo;, &lsquo;Immune system destroying cancer&rsquo;.</p>
<p>BOW, TF-IDF are not ideal for this sentiment analysis problem since they may provide the same outcome with the sentences composed of same words with difference order, like &lsquo;Cancer destroying immune system&rsquo;, &lsquo;Immune system destroying cancer&rsquo;. Thus, We need an algorithm for understanding the context in order to correctly understand human&rsquo;s intent.</p>
<h3 id="word-embedding">Word Embedding<a hidden class="anchor" aria-hidden="true" href="#word-embedding">#</a></h3>
<p>Dimension has its meaning. For example, dog and cat are similar in terms of animal category, i.e. one dimension, but they are not that similar in terms of sound category. The vectorization of previous methods does not take these factors into account. Hence, a systematic transformation of vector space is crucial for extracting relations between words. This mapping process is called embedding. It turns sparse vectors into dense vectors. It resolves issues of lacking connections between words/sentences.</p>
<p>However, creating a good word embedding from scratch is not easy. It needs tons of data to tune the coefficient of matrix(neural network). Adopting pre-trained word embeddings are much more cost-effective for generous use purpose, e.g. building a chatbot in our case. There are several word embeddings, such as Word2Vec, GloVe, fastText, ELMo, and BERT [<a href="https://patil-aakanksha.medium.com/top-5-pre-trained-word-embeddings-20de114bc26">2</a>].</p>
<h3 id="bert">BERT<a hidden class="anchor" aria-hidden="true" href="#bert">#</a></h3>
<p>Bidirectional Encoder Representations from Transformers (BERT) is a multi-layer bidirectional Transformer encoder. The core concepts are Masked Language modelling (MLM) and Next Sentence Prediction (NSP).</p>
<p>MLM enables bidirectional learning from text by masking a word in a sentence. This training technique forces BERT to use words from both sides of masked word to fill the covered word. Interestingly, this is how human usually learns a language by considering the context.</p>
<p>NSP makes BERT understand the relationships between sentences. It is achieved by randomly connecting sentences with non-coherent ones.</p>
<p>Leveraging these two training techniques, the semantic knowledge is captured by the learned embeddings/representations. Making BERT an ideal pre-trained model for solving lots of NLP problems, like semantic analysis, question answering, text prediction, text generation, summarization,&hellip; and so on by adding one additional output layer of neuron network. It achieved new state-of-the-art results and even outperformed human on some tasks [<a href="https://arxiv.org/abs/1810.04805">3</a>].</p>
<p>Hugging face&rsquo;s blog <a href="https://huggingface.co/blog/bert-101">BERT 101</a> explains it quite well and provides tutorial to implement it. Also, the most well-known chatbot is ChatGPT produced by OpenAI nowadays. Here is the <a href="https://chat.openai.com/share/75edac58-6df2-4c51-a52b-95c5193bb586">response</a> from it for answering how MLM works and demonstrating how to implement it.</p>
<h3 id="llm">LLM<a hidden class="anchor" aria-hidden="true" href="#llm">#</a></h3>
<p>A term becomes quite famous recently, since the release of ChatGPT. Large Language Model (LLM), it refers to a more advanced and powerful version of language model which is designed for understanding, generating text. LLM is often addressed extensive scale of training data and model parameter, GPT-3 for example.</p>
<p>LLMs (GPT-3, LlaMA 2, &hellip;) are autogressive models in NLP which generate outputs, one element at a time in a sequential manner. They are based on decoder part of Transformer architecture. It enables LLM to capture long-range dependencies and relationships in the input data. Then providing LLM the ability to generate coherent and contextually relevant text by considering the context of preceding tokens.</p>
<p>In contrast to BERT, LLMs are structured on decoder of Transformer. Without adopting encoder, it is more efficient to train models on large corpus in an autoregressive, unsupervised manner. It then can be fine-tuned for specific tasks. Here are some <a href="https://chat.openai.com/share/87c75338-1458-45f3-b400-541158a05b0a">conversations</a> with chatGPT for teaching me how GPT using decoder of Transformer to accomplish state-of-the-art results. The summarization is amazing for showing the difference between encoder and decoder of Transformer.</p>
<p>The autogressive model learned amount of knowledge on Internet. However, the outputs are mostly not quite useful for human. It hallucinates stuffs that are not existed, or just wrong. Hence, fine-tuned is needed and OpenAI published a paper [<a href="https://arxiv.org/abs/2203.02155">5</a>] about how they get a well-behaved LLM. They introduce a mechanism called reinforcement learning from human feedback (RLHF) to instruct the model. They teach model the knowledge by giving them a dataset of labeler demonstrations of the desired model behavior, fine-tuning the model parameters by supervised learning, and then ranking the results from fine-tuned model, and further fine-tuning it through RLHF.
<img alt="Fine-tuning steps" loading="lazy" src="/images/RLHF.png"></p>
<p>Through these fine-tuning procedure, the model performance boosts hugely even though the model parameters are comparably smaller than model without fine-tuning.</p>
<h2 id="model-evaluation">Model Evaluation<a hidden class="anchor" aria-hidden="true" href="#model-evaluation">#</a></h2>
<p>A query from user can be categorized into several domains, such as emotions, intents, or some specific FAQ dataset. A chatbot can be built based on customized FAQ dataset for providing answers to customers. The performance can be examined as classification problem of machine learning. The procedure is splitting the data into training, validation and test set and, using validation set for tuning hyperparameter and preventing overfitting, evaluating the performance on test set by metrics like accuracy, F1-score (suitable for unbalanced dataset). The similar procedure can be applied to analyze intents, emotions.</p>
<p>If similar search algorithm is used, like building a search engine for getting correct FAQ response or retrieval augmented generation (RAG) system for boosting LLM performance, metrics like hit rate, Mean Reciprocal Rank (MRR) are suitable for examining the model performance.</p>
<h2 id="deployment">Deployment<a hidden class="anchor" aria-hidden="true" href="#deployment">#</a></h2>
<p>Once ideal result from model is generated, it is ready to be deployed to production environment. One of deployment is setting up APIs for serving the model. It either can adopt GET request to pin the API endpoint with queries on url or use POST request to envelope the information and send it to the endpoints as JSON format.</p>
<h2 id="citation">Citation<a hidden class="anchor" aria-hidden="true" href="#citation">#</a></h2>
<p>Cite as</p>
<blockquote>
<p>Ching, Jia-Hau. (Nov, 2023) &ldquo;Creating a Chatbot&rdquo;. JH&rsquo;s Note.</p>
</blockquote>
<p>Or</p>
<pre tabindex="0"><code>@article{ching2023chatbot,
  title   = &#34;Creating a Chatbot&#34;&#34;,
  author  = &#34;Ching, Jia-Hau&#34;,
  journal = &#34;jiahau3.github.io&#34;,
  year    = &#34;2023&#34;,
  month   = &#34;Nov&#34;,
  url     = &#34;https://jiahau3.github.io/posts/chatbot/&#34;
}
</code></pre><h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h2>
<p>[1] TF-IDF definition from <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">Wikipedia</a></p>
<p>[2] Aakanksha Patil <a href="https://patil-aakanksha.medium.com/top-5-pre-trained-word-embeddings-20de114bc26">Top 5 Pre-trained Word Embeddings</a></p>
<p>[3] Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. <a href="https://arxiv.org/abs/1810.04805">&ldquo;Bert: Pre-training of deep bidirectional transformers for language understanding.&rdquo;</a> arXiv preprint arXiv:1810.04805 (2018).</p>
<p>[4] Muller, Britney <a href="https://huggingface.co/blog/bert-101">&ldquo;BERT 101 🤗 State Of The Art NLP Model Explained&rdquo;</a> Hugging Face Blog (2022)</p>
<p>[5] Ouyang et al. <a href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a> arXiv arXiv:2203.02155 (2022)</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/sentiment-analysis/">Sentiment Analysis</a></li>
      <li><a href="http://localhost:1313/tags/word-embedding/">Word Embedding</a></li>
      <li><a href="http://localhost:1313/tags/llm/">LLM</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="http://localhost:1313/posts/recommender_system/">
    <span class="title">Next »</span>
    <br>
    <span>Recommender Systems and Applications of Berlin Grocery Shopping</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Creating a Chatbot on x"
            href="https://x.com/intent/tweet/?text=Creating%20a%20Chatbot&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fchatbot%2f&amp;hashtags=SentimentAnalysis%2cWordEmbedding%2cLLM">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Creating a Chatbot on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fchatbot%2f&amp;title=Creating%20a%20Chatbot&amp;summary=Creating%20a%20Chatbot&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fchatbot%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Creating a Chatbot on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fchatbot%2f&title=Creating%20a%20Chatbot">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Creating a Chatbot on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fchatbot%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Creating a Chatbot on whatsapp"
            href="https://api.whatsapp.com/send?text=Creating%20a%20Chatbot%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fchatbot%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Creating a Chatbot on telegram"
            href="https://telegram.me/share/url?text=Creating%20a%20Chatbot&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fchatbot%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Creating a Chatbot on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Creating%20a%20Chatbot&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fchatbot%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="http://localhost:1313/">JH&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
